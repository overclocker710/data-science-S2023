---
title: "Massachusetts Highway Stops"
author: "Daniel Heitz"
date: 2023-05-04
output:
  github_document:
    toc: true
---

*Purpose*: In this last challenge we'll focus on using logistic regression to study a large, complicated dataset. Interpreting the results of a model can be challenging---both in terms of the statistics and the real-world reasoning---so we'll get some practice in this challenge.

<!-- include-rubric -->
# Grading Rubric
<!-- -------------------------------------------------- -->

Unlike exercises, **challenges will be graded**. The following rubrics define how you will be graded, both on an individual and team basis.

## Individual
<!-- ------------------------- -->

| Category | Needs Improvement | Satisfactory |
|----------|----------------|--------------|
| Effort | Some task __q__'s left unattempted | All task __q__'s attempted |
| Observed | Did not document observations, or observations incorrect | Documented correct observations based on analysis |
| Supported | Some observations not clearly supported by analysis | All observations clearly supported by analysis (table, graph, etc.) |
| Assessed | Observations include claims not supported by the data, or reflect a level of certainty not warranted by the data | Observations are appropriately qualified by the quality & relevance of the data and (in)conclusiveness of the support |
| Specified | Uses the phrase "more data are necessary" without clarification | Any statement that "more data are necessary" specifies which *specific* data are needed to answer what *specific* question |
| Code Styled | Violations of the [style guide](https://style.tidyverse.org/) hinder readability | Code sufficiently close to the [style guide](https://style.tidyverse.org/) |

## Due Date
<!-- ------------------------- -->

All the deliverables stated in the rubrics above are due **at midnight** before the day of the class discussion of the challenge. See the [Syllabus](https://docs.google.com/document/d/1qeP6DUS8Djq_A0HMllMqsSqX3a9dbcx1/edit?usp=sharing&ouid=110386251748498665069&rtpof=true&sd=true) for more information.

*Background*: We'll study data from the [Stanford Open Policing Project](https://openpolicing.stanford.edu/data/), specifically their dataset on Massachusetts State Patrol police stops.

```{r setup}
library(tidyverse)
library(broom)
```

# Setup
<!-- -------------------------------------------------- -->

### __q1__ Go to the [Stanford Open Policing Project](https://openpolicing.stanford.edu/data/) page and download the Massachusetts State Police records in `Rds` format. Move the data to your `data` folder and match the `filename` to load the data.

*Note*: An `Rds` file is an R-specific file format. The function `readRDS` will read these files.

```{r q1-task}
## TODO: Download the data, move to your data folder, and load it
filename <- "yg821jf8611_ma_statewide_2020_04_01.rds"
df_data <- readRDS(filename)
```

# EDA
<!-- -------------------------------------------------- -->

### __q2__ Do your "first checks" on the dataset. What are the basic facts about this dataset?
```{r}
df_data %>% head()
df_data %>% summary()
```


Note that we have both a `subject_race` and `race_Raw` column. There are a few possibilities as to what `race_Raw` represents:

- `race_Raw` could be the race of the police officer in the stop
- `race_Raw` could be an unprocessed version of `subject_race`

Let's try to distinguish between these two possibilities.

### __q3__ Check the set of factor levels for `subject_race` and `raw_Race`. What do you note about overlap / difference between the two sets?

```{r q3-task}
# Check factor levels for subject_race
subject_race_levels <- levels(df_data$subject_race)
print(subject_race_levels)

# Check unique values for race_Raw
race_Raw_unique <- unique(df_data$raw_Race)
print(race_Raw_unique)
```

**Observations**:

- What are the unique values for `subject_race`? asian/pacific islander, black, hispanic, white, other, and unknown
- What are the unique values for `raw_Race`? White, Hispanic, Black, Asian or Pacific Islander, Middle Eastern or East Indian (South Asian), American Indian or Alaskan Native, NA, None - for no operator present citations only, A.
- What is the overlap between the two sets? asian, black, hispanic, and white directly correspond, though with syntactical differences
- What is the difference between the two sets? Middle Eastern is not a category in subject_race. Also None - no operator is not there. I don't know what "A" is. Also there is NA, but that is not a string NA that is a null value meaning the data is absent.

### __q4__ Check whether `subject_race` and `raw_Race` match for a large fraction of cases. Which of the two hypotheses above is most likely, based on your results?

*Note*: Just to be clear, I'm *not* asking you to do a *statistical* hypothesis test.

```{r q4-task}
## TODO: Devise your own way to test the hypothesis posed above.
# Define the mappings between subject_race and raw_Race values
race_mappings <- c("black" = "Black", "asian/pacific islander" = "Asian or Pacific Islander", "hispanic" = "Hispanic", "white" = "White")

# Exclude rows with missing values in subject_race or raw_Race columns
df_data_no_na <- df_data %>%
  filter(!is.na(subject_race) & !is.na(raw_Race))

# Create a new column indicating whether subject_race and raw_Race can be mapped
df_data_no_na <- df_data_no_na %>%
  mutate(race_match = ifelse(race_mappings[subject_race] == raw_Race, TRUE, FALSE))

# Calculate the fraction of rows where subject_race and raw_Race can be mapped (ignoring missing values)
match_fraction <- mean(df_data_no_na$race_match, na.rm = TRUE)
cat("Fraction of rows where subject_race and raw_Race can be mapped:", match_fraction)
```

**Observations**

Between the two hypotheses:

- `race_Raw` could be the race of the police officer in the stop
- `race_Raw` could be an unprocessed version of `subject_race`

which is most plausible, based on your results?

- The second one, they usually match (85%) which suggests race_Raw is unprocessed. And while not definitive evidence, the name race_Raw suggests its raw race data, that has not been processed. Conveniently there is a subject_race column.

## Vis
<!-- ------------------------- -->

### __q5__ Compare the *arrest rate*---the fraction of total cases in which the subject was arrested---across different factors. Create as many visuals (or tables) as you need, but make sure to check the trends across all of the `subject` variables. Answer the questions under *observations* below.

(Note: Create as many chunks and visuals as you need)

```{r}
# Calculate arrest rate by subject_age
arrest_rate_by_age <- df_data %>%
  group_by(subject_age) %>%
  summarize(arrest_rate = mean(arrest_made, na.rm = TRUE))

# Create scatter plot
ggplot(arrest_rate_by_age, aes(x = subject_age, y = arrest_rate)) +
  geom_col() +
  labs(title = "Arrest Rate by Subject Age")

# Calculate arrest rate by subject_sex
arrest_rate_by_sex <- df_data %>%
  group_by(subject_sex) %>%
  summarize(arrest_rate = mean(arrest_made, na.rm = TRUE))

# Create scatter plot
ggplot(arrest_rate_by_sex, aes(x = subject_sex, y = arrest_rate)) +
  geom_col() +
  labs(title = "Arrest Rate by Subject Sex")

# Calculate arrest rate by subject_race
arrest_rate_by_sex <- df_data %>%
  group_by(subject_race) %>%
  summarize(arrest_rate = mean(arrest_made, na.rm = TRUE))

# Create scatter plot
ggplot(arrest_rate_by_sex, aes(x = subject_race, y = arrest_rate)) +
  geom_col() +
  labs(title = "Arrest Rate by Subject Race")
```

**Observations**:

- How does `arrest_rate` tend to vary with `subject_age`?
  - Peaks at 15, goes down at 18, rises again until around 28, then declines until the very end when it jumps around from 0 to something high, I think that's a product of lack of data at the very end but there may be a trend.
- How does `arrest_rate` tend to vary with `subject_sex`?
  - Men have double the arrest rate of women
- How does `arrest_rate` tend to vary with `subject_race`?
  - Hispanic is the highest by far, asian/pacific islander is the lowest (other than unknown). Black is a little higher than white, white is a little higher and asian/PI.

# Modeling
<!-- -------------------------------------------------- -->

We're going to use a model to study the relationship between `subject` factors and arrest rate, but first we need to understand a bit more about *dummy variables*

### __q6__ Run the following code and interpret the regression coefficients. Answer the the questions under *observations* below.

```{r q6-task}
## NOTE: No need to edit; inspect the estimated model terms.
fit_q6 <-
  glm(
    formula = arrest_made ~ subject_age + subject_race + subject_sex,
    data = df_data %>%
      filter(
        !is.na(arrest_made),
        subject_race %in% c("white", "black", "hispanic")
      ),
    family = "binomial"
  )

fit_q6 %>% tidy()
```

**Observations**:

- Which `subject_race` levels are included in fitting the model?
  - All of them
- Which `subject_race` levels have terms in the model?
  - white and hispanic.

You should find that each factor in the model has a level *missing* in its set of terms. This is because R represents factors against a *reference level*: The model treats one factor level as "default", and each factor model term represents a change from that "default" behavior. For instance, the model above treats `subject_sex==male` as the reference level, so the `subject_sexfemale` term represents the *change in probability* of arrest due to a person being female (rather than male).

The this reference level approach to coding factors is necessary for [technical reasons](https://www.andrew.cmu.edu/user/achoulde/94842/lectures/lecture10/lecture10-94842.html#why-is-one-of-the-levels-missing-in-the-regression), but it complicates interpreting the model results. For instance; if we want to compare two levels, neither of which are the reference level, we have to consider the difference in their model coefficients. But if we want to compare all levels against one "baseline" level, then we can relevel the data to facilitate this comparison.

By default `glm` uses the first factor level present as the reference level. Therefore we can use `mutate(factor = fct_relevel(factor, "desired_level"))` to set our `"desired_level"` as the reference factor.

### __q7__ Re-fit the logistic regression from q6 setting `"white"` as the reference level for `subject_race`. Interpret the the model terms and answer the questions below.

```{r q7-task}
## TODO: Re-fit the logistic regression, but set "white" as the reference
## level for subject_race
fit_q7 <-
  glm(
    formula = arrest_made ~ subject_age + relevel(subject_race, ref = "white") + subject_sex,
    data = df_data %>%
      filter(
        !is.na(arrest_made),
        subject_race %in% c("white", "black", "hispanic")
      ),
    family = "binomial"
  )

fit_q7 %>% tidy()
```

**Observations**:

- Which `subject_race` level has the highest probability of being arrested, according to this model? Which has the lowest probability?
  - hispanic has the highest, white has the lowest.
- What could explain this difference in probabilities of arrest across race? List **multiple** possibilities.
  - Different races commmit different amounts of crimes of different types. We cannot say why this would happen based on the data, but assuming the arrests are all made reasonably fairly this drastic of a difference is not simply uncertainty. There are a number of reasons one race could be responsible for more crimes such as poverty, upbringing, or geographic location to name a few. It is also possible that these arrests are unfair. We have no reason to believe they are but simultaneously we have nothing proving that they are so we cannot discount the possibility.
- Look at the sent of variables in the dataset; do any of the columns relate to a potential explanation you listed?
  - type, contraband_found, and reason_for_stop are possible explanations.

One way we can explain differential arrest rates is to include some measure indicating the presence of an arrestable offense. We'll do this in a particular way in the next task.

### __q8__ Re-fit the model using a factor indicating the presence of contraband in the subject's vehicle. Answer the questions under *observations* below.

```{r q8-task}
fit_q8 <-
  glm(
    formula = arrest_made ~ subject_age + subject_race + subject_sex + contraband_found,
    data = df_data %>%
      filter(
        !is.na(arrest_made),
        subject_race %in% c("white", "black", "hispanic")
      ),
    family = "binomial"
  )

fit_q8 %>% tidy()
```

**Observations**:

- How does controlling for found contraband affect the `subject_race` terms in the model?
  - Reduces the variance drastically. This is extremely odd but also fascinating as it shows that race is correlated to likelihood of holding contraband. It doesn't explain all of it, but it roughly zeros the subject_age skew as well as the skew between black and white. Hispanics still have elevated chances even taking this into account but those chances are not as elevated.
- What does the *finding of contraband* tell us about the stop? What does it *not* tell us about the stop?
  - It tells us that the officer searched them and/or the vehicle and found contraband such as drugs, alcohol, or illegal weapons.

### __q9__ Go deeper: Pose at least one more question about the data and fit at least one more model in support of answering that question.

```{r}
fit_q9 <-
  glm(
    formula = arrest_made ~ subject_age + subject_race + subject_sex + vehicle_type,
    data = df_data %>%
      filter(
        !is.na(arrest_made),
        subject_race %in% c("white", "black", "hispanic")
      ),
    family = "binomial"
  )

fit_q9 %>% tidy()
```


**Observations**:

- My question was how vehicle type influences the fit. Commercial is the baseline. Trailers are the least likely to be pulled over, while motorcycles are the most, closely followed by passenger cars. Taxis are well below Commercial but not below trailer. Very interesting. Makes some degree of sense, usually people aren't joyriding or street racing in taxis, delivery trucks, or while towing.

## Further Reading
<!-- -------------------------------------------------- -->

- Stanford Open Policing Project [findings](https://openpolicing.stanford.edu/findings/).
